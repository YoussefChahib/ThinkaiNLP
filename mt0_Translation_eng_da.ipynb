{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_VDaf3XJUgi"
      },
      "source": [
        "###Installing and uploading the needed packages in our project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3L_-Opz3Njq"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pm4hPxuVKQgh"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install sacrebleu\n",
        "!pip install sentencepiece\n",
        "import transformers\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from huggingface_hub import notebook_login\n",
        "from transformers.utils import send_example_telemetry\n",
        "from datasets import load_dataset, load_metric\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset, DatasetDict, load_dataset, load_metric\n",
        "from IPython.display import display, HTML\n",
        "from transformers import AutoTokenizer\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NyngL20LBOK"
      },
      "source": [
        "###Defining our Model: Flan T5 Base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3Dv4jy26adT"
      },
      "outputs": [],
      "source": [
        "#Defining our model :  MT0 base\n",
        "model_checkpoint = \"bigscience/mt0-base\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eItgXRKJK302"
      },
      "source": [
        "###Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_pZoNVJ6x1-"
      },
      "outputs": [],
      "source": [
        "#loading dataset\" Dora it contains more than 10 000 rows\n",
        "df =  pd.read_csv ('./sentences_.csv')\n",
        "metric = load_metric(\"sacrebleu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgO4ybDPeXOS"
      },
      "outputs": [],
      "source": [
        "#Dropping the null rows\n",
        "df = df.dropna()\n",
        "#Trying to reduce the overfitting by addin a string to out 'english column\n",
        "df['english'] = 'translate to arabic : ' + df['english'].astype(str)\n",
        "df['english'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHRO3rDlLVfg"
      },
      "source": [
        "###Spliting data to 2 parts : 80% for training, 20% for test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "US2uV6kd82ki"
      },
      "outputs": [],
      "source": [
        "# split the data into train and test set\n",
        "train, test = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CB3acPruy0J",
        "outputId": "2ac83719-29cc-4ed0-ae8a-bdec146d5b17"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2000, 2)"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtrOpBbxQLeD"
      },
      "source": [
        "###Hugging face dataset load from CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLj8n9Jw7GCS"
      },
      "outputs": [],
      "source": [
        "#Hugging face dataset load from CSV\n",
        "train_ = Dataset.from_pandas(train)\n",
        "test_ = Dataset.from_pandas(test)\n",
        "#Transforming our dataset to the hugging face dictset format\n",
        "df = DatasetDict()\n",
        "#Removing a generated Index column\n",
        "df.remove_columns(\"__index_level_0__\")\n",
        "df['train'] = train_.remove_columns(\"__index_level_0__\")\n",
        "df['test'] = test_.remove_columns(\"__index_level_0__\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmK3i5mNRKrn"
      },
      "source": [
        "###Generalizing random data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zJxrh0CeZZD"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6NLmLLQee4p"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V15dkluDgbJr"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset, DatasetDict, load_dataset, load_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkOddS0j9jIw"
      },
      "outputs": [],
      "source": [
        "#Showing some random examples from our dataset t understand what our dataset looks like\n",
        "def show_random_elements(df, num_examples=5):\n",
        "    assert num_examples <= len(df), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(df)-1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(df)-1)\n",
        "        picks.append(pick)\n",
        "    \n",
        "    data = pd.DataFrame(df[picks])\n",
        "    for column, typ in df.features.items():\n",
        "        if isinstance(typ, datasets.ClassLabel):\n",
        "            data[column] = data[column].transform(lambda i: typ.names[i])\n",
        "    display(HTML(data.to_html()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qxa3aEOW-M7y"
      },
      "outputs": [],
      "source": [
        "show_random_elements(df[\"train\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3i-Uwn8R-DR"
      },
      "source": [
        "###Generating our metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFGzismp94Q4"
      },
      "outputs": [],
      "source": [
        "#Generating a matric\n",
        "metric\n",
        "fake_preds = [\"hello brother\", \"salam khouya\"]\n",
        "fake_labels = [[\"hello brother\"], [\"salam khouya\"]]\n",
        "metric.compute(predictions=fake_preds, references=fake_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-TBgg37RU_L"
      },
      "source": [
        "###Preprocessing data\n",
        "we need to preprocess our data before giving it to our model. \n",
        "We used Transformers Tokenizer tokenize the inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5alCpStaSQ2N"
      },
      "outputs": [],
      "source": [
        "#Preprocessing data   \n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2L88Qy4HTP4J"
      },
      "outputs": [],
      "source": [
        "if \"mbart\" in model_checkpoint:\n",
        "    tokenizer.src_lang = \"en-XX\"\n",
        "    tokenizer.tgt_lang = \"ar_AR\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6lBDtmpTeCu",
        "outputId": "3f215727-52bc-4be7-bb78-b32947278685"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [30273, 261, 714, 1371, 259, 98923, 309, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#testing out tokenizer\n",
        "tokenizer(\"Hello, this one sentence!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HN9zU9dHTsEF"
      },
      "outputs": [],
      "source": [
        "#we need to tokenize them inside the as_target_tokenizer context manager. \n",
        "#This will make sure the tokenizer uses the special tokens corresponding to the targets\n",
        "with tokenizer.as_target_tokenizer():\n",
        "    print(tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JopvFOuDTwSr"
      },
      "outputs": [],
      "source": [
        "#T5 checkpoints require a special prefix to put before the inputs\n",
        "if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n",
        "    prefix = \"translate English to darija: \"\n",
        "else:\n",
        "    prefix = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SfVGyQLT1jE"
      },
      "outputs": [],
      "source": [
        "#the function that will preprocess our samples\n",
        "max_input_length = 128\n",
        "max_target_length = 128\n",
        "source_lang = \"english\"\n",
        "target_lang = \"darija\"\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    print(examples)\n",
        "    inputs = [prefix + ex  for ex in examples[source_lang]]\n",
        "    targets = [ex for ex in examples[target_lang]]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
        "\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTZ90aJ2WnUU"
      },
      "outputs": [],
      "source": [
        "show_random_elements(df[\"train\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znvK9K6UT9aZ"
      },
      "source": [
        "To apply the preprocess function on all the pairs of sentences in our dataset, we just use the map method of our dataset object we created earlier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RDRZirnUVRb"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets = df.map(preprocess_function, batched=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdoVpuGfUDMz"
      },
      "source": [
        "###Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYCpjj1X_mJe"
      },
      "outputs": [],
      "source": [
        "#Finetuning\n",
        "#we use the AutoModelForSeq2SeqLM class. Like with the tokenizer.\n",
        "#the from_pretrained method will download and cache the model for us.\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sssXQYbUitG"
      },
      "source": [
        "To instantiate a Seq2SeqTrainer, we will need to define three more things. The most important is the Seq2SeqTrainingArguments, which is a class that contains all the attributes to customize the training. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VcR5Jgj_sqO"
      },
      "outputs": [],
      "source": [
        "source_lang = \"english\"\n",
        "target_lang = \"darija\"\n",
        "batch_size = 16\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    f\"{model_name}-finetuned-{source_lang}-to-{target_lang}\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=5,\n",
        "    predict_with_generate=True,\n",
        "    fp16=False,\n",
        "    push_to_hub=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCpJFfNg9-m5",
        "outputId": "0a3bce5c-3112-416b-f948-019ab99271a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hY3rGRsfAPvV"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRBqrdXGUycQ"
      },
      "source": [
        "The last thing to define for our Seq2SeqTrainer is how to compute the metrics from the predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcaYAHJeAUo_"
      },
      "outputs": [],
      "source": [
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "\n",
        "    return preds, labels\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Some simple post-processing\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"bleu\": result[\"score\"]}\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5Vd2o81Uwzu"
      },
      "source": [
        "we need to pass all of this along with our datasets to the Seq2SeqTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Af1nhp-Ajkm"
      },
      "outputs": [],
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HP7QB3CVItF"
      },
      "source": [
        "We can now finetune our model by just calling the train method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWIyO5XsAyQf"
      },
      "outputs": [],
      "source": [
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mkjhbf4yVZn_"
      },
      "source": [
        "###Testing with new data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMO4TZwMbMYu"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(\"translate to arabic : hi, I am salma and you?\", return_tensors=\"pt\").to(\"cuda\")\n",
        "output = model.generate(**inputs)\n",
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoXPUJ5vomJP"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(output.cpu().numpy()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5Y6IdiFsxdy"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(\"translate to arabic : hello, would you like to give a pitch tomorrow?\", return_tensors=\"pt\").to(\"cuda\")\n",
        "output = model.generate(**inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OHygtQJq5st"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(output.cpu().numpy()[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BELtO0Dts-Ea"
      },
      "source": [
        "Gradio-Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3BRI88JtHU6"
      },
      "outputs": [],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdKhlfc-8Y2L"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daN3wsU9vU2P"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqY_Z2jF1bNy"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install diffusers==0.12.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "h-UfzHsmq7Gc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import gradio as gr\n",
        "import time\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WItstv1ztC85"
      },
      "outputs": [],
      "source": [
        "def translation(text):\n",
        "\n",
        "    model_checkpoint = \"bigscience/mt0-base\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
        "    #inference\n",
        "    inputs = tokenizer(\"translate to arabic: \" + text, return_tensors=\"pt\")\n",
        "    output = model.generate(**inputs)\n",
        "    output = tokenizer.decode(output.cpu().numpy()[0], skip_special_tokens=True)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translation('how are you doing today ?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 58
        },
        "id": "tRu-y9aOTtbL",
        "outputId": "c9a9a369-6155-41c5-bbcc-5176e42c51e5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'كيف فعلت اليوم؟'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoFCM73nu_wm"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    print('\\tinit models')\n",
        "\n",
        "    #inputs = [gr.inputs.Radio(['nllb-distilled-600M', 'nllb-1.3B', 'nllb-distilled-1.3B'], label='NLLB Model'),\n",
        "    inputs = [gr.inputs.Textbox(lines=5, label=\"Input text\")]\n",
        "\n",
        "    outputs = gr.outputs.Textbox(label=\"Output text\")\n",
        "\n",
        "    title = \"Derej M3aya\"\n",
        "\n",
        "    demo_status = \"Demo is running on CPU\"\n",
        "    description = f\"Details: https://github.com/facebookresearch/fairseq/tree/nllb. {demo_status}\"\n",
        "    examples = [\n",
        "    ['English', 'Darija', 'Hi nice to meet you']\n",
        "    ]\n",
        "\n",
        "    gr.Interface(translation,\n",
        "                 inputs,\n",
        "                 outputs,\n",
        "                 title=title,\n",
        "                 description=description,\n",
        "                 ).launch(share=True, debug = True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2OikGai6UEo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}