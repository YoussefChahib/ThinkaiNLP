{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Installing and uploading the needed packages in our project"
      ],
      "metadata": {
        "id": "-_VDaf3XJUgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n"
      ],
      "metadata": {
        "id": "o3L_-Opz3Njq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install sacrebleu\n",
        "!pip install sentencepiece\n",
        "import transformers\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from huggingface_hub import notebook_login\n",
        "from transformers.utils import send_example_telemetry\n",
        "from datasets import load_dataset, load_metric\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset, DatasetDict, load_dataset, load_metric\n",
        "from IPython.display import display, HTML\n",
        "from transformers import AutoTokenizer\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n"
      ],
      "metadata": {
        "id": "pm4hPxuVKQgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#notebook_login()"
      ],
      "metadata": {
        "id": "Db5MAAauBroF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#send_example_telemetry(\"translation_notebook\", framework=\"pytorch\")"
      ],
      "metadata": {
        "id": "ibjmE5mV6C9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Defining our Model: Flan T5 Base"
      ],
      "metadata": {
        "id": "8NyngL20LBOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining our model :  Flan T5 Base\n",
        "model_checkpoint = \"google/flan-t5-base\"\n"
      ],
      "metadata": {
        "id": "P3Dv4jy26adT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Loading the dataset"
      ],
      "metadata": {
        "id": "eItgXRKJK302"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#loading dataset\" Dora it contains more than 10 000 rows\n",
        "df =  pd.read_csv ('./sentences_.csv')\n",
        "metric = load_metric(\"sacrebleu\")"
      ],
      "metadata": {
        "id": "S_pZoNVJ6x1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dropping the null rows\n",
        "df = df.dropna()\n",
        "#Trying to reduce the overfitting by addin a string to out 'english column\n",
        "df['english'] = 'translate to arabic : ' + df['english'].astype(str)\n",
        "df['english'].head()"
      ],
      "metadata": {
        "id": "VgO4ybDPeXOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Spliting data to 2 parts : 80% for training, 20% for test"
      ],
      "metadata": {
        "id": "wHRO3rDlLVfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data into train and test set\n",
        "train, test = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)"
      ],
      "metadata": {
        "id": "US2uV6kd82ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Hugging face dataset load from CSV"
      ],
      "metadata": {
        "id": "KtrOpBbxQLeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Hugging face dataset load from CSV\n",
        "train_ = Dataset.from_pandas(train)\n",
        "test_ = Dataset.from_pandas(test)\n",
        "#Transforming our dataset to the hugging face dictset format\n",
        "df = DatasetDict()\n",
        "#Removing a generated Index column\n",
        "df.remove_columns(\"__index_level_0__\")\n",
        "df['train'] = train_.remove_columns(\"__index_level_0__\")\n",
        "df['test'] = test_.remove_columns(\"__index_level_0__\")\n"
      ],
      "metadata": {
        "id": "wLj8n9Jw7GCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Generalizing random data "
      ],
      "metadata": {
        "id": "cmK3i5mNRKrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Showing some random examples from our dataset t understand what our dataset looks like\n",
        "def show_random_elements(df, num_examples=5):\n",
        "    assert num_examples <= len(df), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(df)-1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(df)-1)\n",
        "        picks.append(pick)\n",
        "    \n",
        "    data = pd.DataFrame(df[picks])\n",
        "    for column, typ in df.features.items():\n",
        "        if isinstance(typ, datasets.ClassLabel):\n",
        "            data[column] = data[column].transform(lambda i: typ.names[i])\n",
        "    display(HTML(data.to_html()))"
      ],
      "metadata": {
        "id": "YkOddS0j9jIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_random_elements(df[\"train\"])"
      ],
      "metadata": {
        "id": "Qxa3aEOW-M7y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "43e11204-bda5-4d9e-b2f7-285c57065af2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>english</th>\n",
              "      <th>darija</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>translate to arabic : Or are you just saying that to make fun of me?</td>\n",
              "      <td>olla tatgoul dakchi hir bach tD7ek 3lia?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>translate to arabic : I can't think of anything worse than a rubbish band playing all night</td>\n",
              "      <td>man9drch nfakar fchi haja khyeb mn jo9 dya zbal kayghnniw llil kaml</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>translate to arabic : He is crying</td>\n",
              "      <td>kaybki</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>translate to arabic : I want to leave</td>\n",
              "      <td>bghit nmchi f7alli</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>translate to arabic : Careful, there's roots everywhere</td>\n",
              "      <td>reddo lbal, rah kaynin jjdoura finmma mchiti</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Generating our metric"
      ],
      "metadata": {
        "id": "Y3i-Uwn8R-DR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Generating a matric\n",
        "metric\n",
        "fake_preds = [\"hello brother\", \"salam khouya\"]\n",
        "fake_labels = [[\"hello brother\"], [\"salam khouya\"]]\n",
        "metric.compute(predictions=fake_preds, references=fake_labels)"
      ],
      "metadata": {
        "id": "UFGzismp94Q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Preprocessing data\n",
        "we need to preprocess our data before giving it to our model. \n",
        "We used Transformers Tokenizer tokenize the inputs."
      ],
      "metadata": {
        "id": "4-TBgg37RU_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocessing data   \n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "5alCpStaSQ2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if \"mbart\" in model_checkpoint:\n",
        "    tokenizer.src_lang = \"en-XX\"\n",
        "    tokenizer.tgt_lang = \"ar_AR\""
      ],
      "metadata": {
        "id": "2L88Qy4HTP4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing out tokenizer\n",
        "tokenizer(\"Hello, this one sentence!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6lBDtmpTeCu",
        "outputId": "502a74c2-0e6d-4c68-902a-151c091e243b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [8774, 6, 48, 80, 7142, 55, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we need to tokenize them inside the as_target_tokenizer context manager. \n",
        "#This will make sure the tokenizer uses the special tokens corresponding to the targets\n",
        "with tokenizer.as_target_tokenizer():\n",
        "    print(tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"]))"
      ],
      "metadata": {
        "id": "HN9zU9dHTsEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#T5 checkpoints require a special prefix to put before the inputs\n",
        "if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n",
        "    prefix = \"translate English to darija: \"\n",
        "else:\n",
        "    prefix = \"\""
      ],
      "metadata": {
        "id": "JopvFOuDTwSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the function that will preprocess our samples\n",
        "max_input_length = 128\n",
        "max_target_length = 128\n",
        "source_lang = \"english\"\n",
        "target_lang = \"darija\"\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    print(examples)\n",
        "    inputs = [prefix + ex  for ex in examples[source_lang]]\n",
        "    targets = [ex for ex in examples[target_lang]]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
        "\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "8SfVGyQLT1jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_random_elements(df[\"train\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "nTZ90aJ2WnUU",
        "outputId": "04cd28c7-aae3-4b00-df36-ee911a5da515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>english</th>\n",
              "      <th>darija</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>translate to arabic : Did you eat any of those mushrooms?</td>\n",
              "      <td>wach kliti chi w7da mn had champignons?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>translate to arabic : Don't overheat them.</td>\n",
              "      <td>matskhenhomch bzzaf.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>translate to arabic : we need solution</td>\n",
              "      <td>khasna 7all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>translate to arabic : Yes, of course!</td>\n",
              "      <td>ah darori!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>translate to arabic : I am just pulling your leg</td>\n",
              "      <td>kent ghir kantfella m3ak</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To apply the preprocess function on all the pairs of sentences in our dataset, we just use the map method of our dataset object we created earlier"
      ],
      "metadata": {
        "id": "znvK9K6UT9aZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = df.map(preprocess_function, batched=True)\n"
      ],
      "metadata": {
        "id": "4RDRZirnUVRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Finetuning"
      ],
      "metadata": {
        "id": "KdoVpuGfUDMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Finetuning\n",
        "#we use the AutoModelForSeq2SeqLM class. Like with the tokenizer.\n",
        "#the from_pretrained method will download and cache the model for us.\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "aYCpjj1X_mJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To instantiate a Seq2SeqTrainer, we will need to define three more things. The most important is the Seq2SeqTrainingArguments, which is a class that contains all the attributes to customize the training. "
      ],
      "metadata": {
        "id": "9sssXQYbUitG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_lang = \"english\"\n",
        "target_lang = \"darija\"\n",
        "batch_size = 16\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    f\"{model_name}-finetuned-{source_lang}-to-{target_lang}\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=5,\n",
        "    predict_with_generate=True,\n",
        "    fp16=False,\n",
        "    push_to_hub=False,\n",
        ")"
      ],
      "metadata": {
        "id": "5VcR5Jgj_sqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ],
      "metadata": {
        "id": "hY3rGRsfAPvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last thing to define for our Seq2SeqTrainer is how to compute the metrics from the predictions."
      ],
      "metadata": {
        "id": "LRBqrdXGUycQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "\n",
        "    return preds, labels\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Some simple post-processing\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"bleu\": result[\"score\"]}\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    return result"
      ],
      "metadata": {
        "id": "ZcaYAHJeAUo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we need to pass all of this along with our datasets to the Seq2SeqTrainer"
      ],
      "metadata": {
        "id": "Q5Vd2o81Uwzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "5Af1nhp-Ajkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now finetune our model by just calling the train method"
      ],
      "metadata": {
        "id": "8HP7QB3CVItF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "kWIyO5XsAyQf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "d7c1c0d4-54ad-4b35-f6e2-ea62ef2699d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2500/2500 22:38, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Bleu</th>\n",
              "      <th>Gen Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>4.092500</td>\n",
              "      <td>3.481138</td>\n",
              "      <td>0.515200</td>\n",
              "      <td>15.920500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.545600</td>\n",
              "      <td>3.240424</td>\n",
              "      <td>0.640900</td>\n",
              "      <td>15.955000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.372800</td>\n",
              "      <td>3.116794</td>\n",
              "      <td>0.987900</td>\n",
              "      <td>16.162000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.279900</td>\n",
              "      <td>3.058258</td>\n",
              "      <td>1.006300</td>\n",
              "      <td>16.069000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.244500</td>\n",
              "      <td>3.039718</td>\n",
              "      <td>0.890300</td>\n",
              "      <td>16.235000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2500, training_loss=3.507073388671875, metrics={'train_runtime': 1359.4898, 'train_samples_per_second': 29.408, 'train_steps_per_second': 1.839, 'total_flos': 1355455780503552.0, 'train_loss': 3.507073388671875, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 201
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Testing with new data"
      ],
      "metadata": {
        "id": "Mkjhbf4yVZn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\"translate to arabic : Did you eat any of those mushrooms?\", return_tensors=\"pt\").to(\"cuda\")\n",
        "output = model.generate(**inputs)\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMO4TZwMbMYu",
        "outputId": "4b7b82bc-6034-443c-9ede-11d56ed22242"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[    0,     3, 10674,     3,    29,    17,     9,     3,    40,    23,\n",
              "             9,     3,    40,   519,     9,    26,     3,    40,   519,     9]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(output.cpu().numpy()[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 58
        },
        "id": "ZoXPUJ5vomJP",
        "outputId": "460470cd-21be-4427-8fbd-8e77c4df4344"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<pad> wach nta lia l3ad l3a'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 203
        }
      ]
    }
  ]
}
